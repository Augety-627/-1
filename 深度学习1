# ====================== æ•°æ®é¢„å¤„ç† ======================
import os
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from tqdm import tqdm  # ç”¨äºæ˜¾ç¤ºè®­ç»ƒè¿›åº¦æ¡
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split

# è¯»å–æ•°æ®
bike_df = pd.read_csv('/Users/by-augety/Desktop/éå¸¸é‡è¦ğŸ“/è¿›åº¦/å…±äº«å•è½¦æ•°æ®12.csv')

# ====================== æ—¶é—´/äº‹ä»¶ç‰¹å¾å·¥ç¨‹ ======================
# è½¬æ¢ä¸º datetime å¹¶æ’åº
bike_df['datetime'] = pd.to_datetime(bike_df['end_date'] + ' ' + bike_df['end_time'])
bike_df = bike_df.sort_values(by='datetime').reset_index(drop=True)

# æ·»åŠ æ—¶é—´ç‰¹å¾
def add_time_features(df):
    """ä¸ºæ•°æ®é›†æ·»åŠ æ—¶é—´ç›¸å…³çš„ç‰¹å¾"""
    # æ—¶é—´ç¼–ç 
    df['hour_sin'] = np.sin(2 * np.pi * df['datetime'].dt.hour / 24)
    df['hour_cos'] = np.cos(2 * np.pi * df['datetime'].dt.hour / 24)
    df['day_of_week'] = df['datetime'].dt.dayofweek
    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
    df['month_sin'] = np.sin(2 * np.pi * df['datetime'].dt.month / 12)
    df['month_cos'] = np.cos(2 * np.pi * df['datetime'].dt.month / 12)
    # æ—¶é—´é—´éš”
    df['delta_t'] = df['datetime'].diff().dt.total_seconds().div(60).fillna(0)
    return df

bike_df = add_time_features(bike_df)

# ç‹¬çƒ­ç¼–ç 
bike_df = pd.get_dummies(bike_df, columns=['end_subway_type'], prefix='subway_type')

# æ·»åŠ æ»‘åŠ¨çª—å£ç»Ÿè®¡ç‰¹å¾
def add_window_statistics(df, window_size=15):
    """ä¸ºæ»‘åŠ¨çª—å£æ·»åŠ ç»Ÿè®¡ç‰¹å¾"""
    df['window_demand_mean'] = df['Arrive_demand_15mins'].rolling(
        window=window_size, min_periods=1, closed='left'
    ).mean()
    df['window_demand_std'] = df['Arrive_demand_15mins'].rolling(
        window=window_size, min_periods=1, closed='left'
    ).std().fillna(0)
    return df

bike_df = add_window_statistics(bike_df)

# å®šä¹‰ç‰¹å¾åˆ—
features = [
    'month_sin', 'month_cos', 'day_sin', 'day_cos', 'is_weekend', 'hour_sin', 'hour_cos',
    'is_peak', 'end_temperature', 'end_wind_speed', 'end_precipitation', 'end_humidity',
    'window_demand_mean', 'window_demand_std', 'subway_type_å±…ä½å‹', 'subway_type_äº¤é€šå‹',
    'subway_type_æ··åˆå‹', 'subway_type_äº§ä¸šå‹', 'subway_type_å•†æœå‹', 'subway_type_å…¬å…±å‹'
]

# ====================== åˆ’åˆ†æ•°æ®é›† ======================
# æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†æ•°æ®é›†
total_samples = len(bike_df)
train_end = int(total_samples * 0.7)  # è®­ç»ƒé›†ç»“æŸä½ç½®
val_end = train_end + int(total_samples * 0.15)  # éªŒè¯é›†ç»“æŸä½ç½®

# åˆ’åˆ†æ•°æ®é›†
train_df = bike_df.iloc[:train_end].copy()
val_df = bike_df.iloc[train_end:val_end].copy()
test_df = bike_df.iloc[val_end:].copy()

# ====================== æ ‡å‡†åŒ–è¿ç»­ç‰¹å¾ ======================
# å®šä¹‰è¿ç»­ç‰¹å¾
cont_features = ['end_temperature', 'end_wind_speed', 'end_precipitation', 'end_humidity']

# åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨
scaler_arrive = StandardScaler()

# åªåœ¨è®­ç»ƒé›†ä¸Š fitï¼Œå…¶ä»–é›†ç”¨ç›¸åŒå‚æ•° transform
train_df[cont_features] = scaler_arrive.fit_transform(train_df[cont_features])
val_df[cont_features] = scaler_arrive.transform(val_df[cont_features])
test_df[cont_features] = scaler_arrive.transform(test_df[cont_features])

# ç›®æ ‡å€¼æ ‡å‡†åŒ–
target_scaler = MinMaxScaler()
train_df['scaled_arrive_demand'] = target_scaler.fit_transform(train_df[['Arrive_demand_15mins']])
val_df['scaled_arrive_demand'] = target_scaler.transform(val_df[['Arrive_demand_15mins']])
test_df['scaled_arrive_demand'] = target_scaler.transform(test_df[['Arrive_demand_15mins']])

# ====================== æ»‘åŠ¨çª—å£ç”Ÿæˆ ======================
def generate_sequences(df, window_size):
    """ç”Ÿæˆæ»‘åŠ¨çª—å£åºåˆ—"""
    X_sequences = []
    y_targets = []
    for i in range(window_size, len(df)):
        window_data = df.iloc[i - window_size : i][features].values
        window_data = window_data.astype(np.float32)  # æ˜¾å¼è½¬æ¢ä¸º float32
        X_sequences.append(window_data)
        y_targets.append(df.iloc[i]['scaled_arrive_demand'])
    
    # è½¬æ¢ä¸º NumPy æ•°ç»„
    X_sequences = np.array(X_sequences, dtype=np.float32)
    y_targets = np.array(y_targets, dtype=np.float32)
    
    # æ£€æŸ¥å¹¶å¤„ç†æ— æ•ˆå€¼
    if np.isnan(X_sequences).any() or np.isinf(X_sequences).any():
        X_sequences = np.nan_to_num(X_sequences, nan=0.0, posinf=0.0, neginf=0.0)
    if np.isnan(y_targets).any() or np.isinf(y_targets).any():
        y_targets = np.nan_to_num(y_targets, nan=0.0, posinf=0.0, neginf=0.0)
    
    return X_sequences, y_targets

# ç”Ÿæˆè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†çš„çª—å£
WINDOW_SIZE = 15
X_train, y_train = generate_sequences(train_df, WINDOW_SIZE)
X_val, y_val = generate_sequences(val_df, WINDOW_SIZE)
X_test, y_test = generate_sequences(test_df, WINDOW_SIZE)

# ====================== è½¬æ¢ä¸ºå¼ é‡ ======================
# è½¬æ¢ä¸º PyTorch å¼ é‡
X_train_tensor = torch.FloatTensor(X_train)
y_train_tensor = torch.FloatTensor(y_train)
X_val_tensor = torch.FloatTensor(X_val)
y_val_tensor = torch.FloatTensor(y_val)
X_test_tensor = torch.FloatTensor(X_test)
y_test_tensor = torch.FloatTensor(y_test)

# åˆ›å»º DataLoader
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# ====================== æ¨¡å‹å®šä¹‰ ======================
class LSTM_Model(nn.Module):
    """åŸºç¡€ LSTM æ¨¡å‹"""
    def __init__(self, input_size, hidden_size=256, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, dropout=0.5, batch_first=True)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 1)
        )
    
    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º

class BiLSTM_Model(nn.Module):
    """åŒå‘ LSTM æ¨¡å‹"""
    def __init__(self, input_size, hidden_size=256, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, dropout=0.5, batch_first=True, bidirectional=True)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size * 2, 128),  # åŒå‘ LSTM è¾“å‡ºç»´åº¦æ˜¯ hidden_size * 2
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 1)
        )
    
    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])

class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, hidden_size, num_heads=4):
        super().__init__()
        self.num_heads = num_heads
        self.head_size = hidden_size // num_heads
        self.query = nn.Linear(hidden_size, hidden_size)
        self.key = nn.Linear(hidden_size, hidden_size)
        self.value = nn.Linear(hidden_size, hidden_size)
        self.fc = nn.Linear(hidden_size, hidden_size)  # æœ€ç»ˆè¾“å‡ºæ³¨æ„åŠ›æƒé‡

    def forward(self, lstm_out):
        batch_size, seq_len, _ = lstm_out.shape
        # åˆ†å‰²ä¸ºå¤šä¸ªå¤´
        Q = self.query(lstm_out).view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)
        K = self.key(lstm_out).view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)
        V = self.value(lstm_out).view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)
        
        # è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_size, dtype=torch.float32))
        attn_weights = torch.softmax(scores, dim=-1)
        
        # åŠ æƒæ±‚å’Œ
        context = torch.matmul(attn_weights, V).transpose(1, 2).contiguous().view(batch_size, seq_len, -1)
        return context

class LSTM_AM_Model(nn.Module):
    """LSTM + æ³¨æ„åŠ›æœºåˆ¶æ¨¡å‹"""
    def __init__(self, input_size, hidden_size=256, num_layers=2, num_heads=4):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, dropout=0.5, batch_first=True)
        self.attention = MultiHeadAttention(hidden_size, num_heads)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 1)
        )
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        attn_out = self.attention(lstm_out)
        return self.fc(attn_out[:, -1, :])  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º

class BiLSTM_AM_Model(nn.Module):
    """åŒå‘ LSTM + æ³¨æ„åŠ›æœºåˆ¶æ¨¡å‹"""
    def __init__(self, input_size, hidden_size=256, num_layers=2, num_heads=4):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, dropout=0.5, batch_first=True, bidirectional=True)
        self.attention = MultiHeadAttention(hidden_size * 2, num_heads)  # åŒå‘ LSTM è¾“å‡ºç»´åº¦æ˜¯ hidden_size * 2
        self.fc = nn.Sequential(
            nn.Linear(hidden_size * 2, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 1)
        )
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        attn_out = self.attention(lstm_out)
        return self.fc(attn_out[:, -1, :])

# ====================== è®­ç»ƒå‡½æ•° ======================
def train_model(model, train_loader, val_loader, epochs, patience):
    """è®­ç»ƒå‡½æ•°ï¼Œæ”¯æŒæ—©åœæœºåˆ¶"""
    model = model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
    criterion = nn.L1Loss()  # ä½¿ç”¨ MAE ä½œä¸ºæŸå¤±å‡½æ•°
    
    best_loss = float('inf')
    best_epoch = 0
    patience_counter = 0
    
    # è®°å½•è®­ç»ƒå’ŒéªŒè¯æŸå¤±
    train_losses = []
    val_losses = []
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for X_batch, y_batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            outputs = model(X_batch).squeeze()
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * X_batch.size(0)
        
        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        train_loss = train_loss / len(train_loader.dataset)
        train_losses.append(train_loss)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_val_batch, y_val_batch in val_loader:
                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)
                outputs = model(X_val_batch).squeeze()
                val_loss += criterion(outputs, y_val_batch).item() * X_val_batch.size(0)
        
        # è®¡ç®—å¹³å‡éªŒè¯æŸå¤±
        val_loss = val_loss / len(val_loader.dataset)
        val_losses.append(val_loss)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_loss:
            best_loss = val_loss
            best_epoch = epoch
            patience_counter = 0
            torch.save(model.state_dict(), f'best_{model.__class__.__name__}.pth')
        else:
            patience_counter += 1
        
        print(f"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
        
        # æ—©åœæœºåˆ¶
        if patience_counter >= patience:
            print(f"Early stopping triggered at epoch {epoch+1}")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load(f'best_{model.__class__.__name__}.pth'))
    print(f"Best model from epoch {best_epoch+1} with Val Loss: {best_loss:.4f} loaded.")
    
    return model

# ====================== æ¨¡å‹è®­ç»ƒæ‰§è¡Œ ======================
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# å®šä¹‰æ‰€æœ‰æ¨¡å‹
models = {
    'LSTM': LSTM_Model(input_size=len(features)),
    'LSTM-AM': LSTM_AM_Model(input_size=len(features)),
    'BiLSTM': BiLSTM_Model(input_size=len(features)),
    'BiLSTM-AM': BiLSTM_AM_Model(input_size=len(features))
}

# è®­ç»ƒæ‰€æœ‰æ¨¡å‹
for model_name, model in models.items():
    print(f"\n===== è®­ç»ƒ {model_name} æ¨¡å‹ =====")
    trained_model = train_model(
        model, 
        train_loader, 
        val_loader, 
        epochs=100, 
        patience=10
    )
    torch.save(trained_model.state_dict(), f'final_{model_name}_model.pth')

# ====================== æ¨¡å‹è¯„ä¼°å‡½æ•° ======================
def evaluate_model(model, test_loader, target_scaler):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œè®¡ç®— MAEã€MSEã€RMSEã€MAPE å’Œ RÂ²"""
    model.eval()
    y_true = []
    y_pred = []
    
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch).squeeze()
            y_true.extend(y_batch.cpu().numpy())
            y_pred.extend(outputs.cpu().numpy())
    
    # åæ ‡å‡†åŒ–ç›®æ ‡å€¼
    y_true = target_scaler.inverse_transform(np.array(y_true).reshape(-1, 1)).flatten()
    y_pred = target_scaler.inverse_transform(np.array(y_pred).reshape(-1, 1)).flatten()
    
    # è®¡ç®—æŒ‡æ ‡
    mae = mean_absolute_error(y_true, y_pred)  # MAE
    mse = mean_squared_error(y_true, y_pred)  # MSE
    rmse = np.sqrt(mse)  # RMSE
    r2 = r2_score(y_true, y_pred)  # RÂ²
    
    # è®¡ç®— MAPEï¼ˆé¿å…é™¤ä»¥é›¶ï¼‰
    nonzero_mask = y_true != 0  # ä»…å¯¹éé›¶å€¼è®¡ç®— MAPE
    mape = np.mean(np.abs((y_true[nonzero_mask] - y_pred[nonzero_mask]) / y_true[nonzero_mask])) * 100  # MAPEï¼ˆç™¾åˆ†æ¯”å½¢å¼ï¼‰
    
    return {
        'MAE': mae,
        'MSE': mse,
        'RMSE': rmse,
        'MAPE': mape,
        'R2': r2
    }

# ====================== æ¨¡å‹è¯„ä¼°æ‰§è¡Œ ======================
# åŠ è½½æ‰€æœ‰æ¨¡å‹çš„æœ€ä½³æƒé‡
for model_name, model in models.items():
    model.load_state_dict(torch.load(f'final_{model_name}_model.pth'))
    print(f"\n===== è¯„ä¼° {model_name} æ¨¡å‹ =====")
    metrics = evaluate_model(model, test_loader, target_scaler)
    print(f"MAE: {metrics['MAE']:.4f}")
    print(f"MSE: {metrics['MSE']:.4f}")
    print(f"RMSE: {metrics['RMSE']:.4f}")
    print(f"MAPE: {metrics['MAPE']:.4f}%")
    print(f"RÂ²: {metrics['R2']:.4f}")
